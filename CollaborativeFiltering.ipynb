{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import iicf_prediction_utils as iicfp\n",
    "import iicf_similarity_matrix_utils as iicfsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare MovieLens 1M Dataset\n",
    "\n",
    "ratings = np.loadtxt('./movielens-1m/ml-1m/ratings.dat', dtype='int', delimiter='::')\n",
    "\n",
    "movie_ids = np.array(sorted(set(ratings[:, 1])))\n",
    "user_ids = np.array(sorted(set(ratings[:, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = np.zeros((len(user_ids), len(movie_ids)), dtype='int')\n",
    "\n",
    "for ui, u in enumerate(user_ids):\n",
    "    ratings_for_u = ratings[ratings[:, 0] == u]\n",
    "    for ru in ratings_for_u:\n",
    "        dat[ui, np.where(movie_ids == ru[1])[0][0]] = ru[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering (CF) Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Predictors\n",
    "\n",
    "* do not depend on the user's ratings. Hence, **Non-personalised** baselines.\n",
    "* can be used for **Pre-processing** and **Normalising** data for use with personalised algorithms.\n",
    "* can be used to get over **Cold-start** problem for new users and items.\n",
    "\n",
    "--------------------\n",
    "\n",
    "$U$ : set of users\n",
    "\n",
    "$I$ : set of items\n",
    "\n",
    "$\\textbf{R}$ : ratings matrix\n",
    "\n",
    "$U_i$ : set of users that rate for item $\\textit{i}$.\n",
    "\n",
    "$I_u$ : set of items that are rated by user $\\textit{u}$. \n",
    "\n",
    "$r_{u,i}$ : rating of user $\\textit{u}$ for item $\\textit{i}$.\n",
    "\n",
    "$b_{u,i}$ : baseline prediction for user $\\textit{u}$ and item $\\textit{i}$.\n",
    "\n",
    "### Simplest Baseline Method\n",
    "\n",
    "\n",
    "$b_{u,i} = \\mu = \\frac{1}{|U|} \\sum_{u} \\frac{1}{|I_u|} \\sum_{i \\in I_u} r_{u,i}$ : average rating over all ratings in the system.\n",
    "\n",
    "$b_{u,i} = \\hat{r_u} = \\frac{1}{|I_u|} \\sum_{i \\in I_u} r_{u,i}$ : average rating for user $\\textit{u}$.\n",
    "\n",
    "$b_{u,i} = \\hat{r_i} = \\frac{1}{|U_i|} \\sum_{u \\in U_i} r_{u,i}$ : average rating for item $\\textit{i}$.\n",
    "\n",
    "### General Baseline Method\n",
    "\n",
    "* combines the **user average rating** ($\\hat{r_u} = \\mu + b_u$) with the **average deviation** ($b_i$) from user mean rating for a particular item [14, 58, 79, 110, 115].\n",
    "\n",
    "-----------------\n",
    "\n",
    "$b_u$ : user baseline predictor.\n",
    "\n",
    "$b_i$ : item baseline predictor.\n",
    "\n",
    "-----------------\n",
    "\n",
    "$b_{u,i} = \\mu + b_u + b_i$\n",
    "\n",
    "$b_u = \\frac{1}{|I_u|} \\sum_{i \\in I_u} (r_{u,i} - \\mu) = \\hat{r_u} - \\mu$\n",
    "\n",
    "$b_i = \\frac{1}{|U_i|} \\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu) = \\hat{r_i} - \\mu - \\frac{1}{|U_i|} \\sum_{u \\in U_i} b_u = \\hat{r_i} - \\frac{1}{|U_i|} \\sum_{u \\in U_i} \\hat{r_u}$\n",
    "\n",
    "Hence\n",
    "\n",
    "$b_{u,i} = \\hat{r_i} + \\hat{r_u} - \\frac{1}{|U_i|} \\sum_{u' \\in U_i} \\hat{r_{u'}}$\n",
    "\n",
    "------------------\n",
    "\n",
    "* can be further regularised, providing a more reasonable estimate of user and item preferences in the face of sparse sampling, with the incorporation of **damping terms** $\\beta_u$ and $\\beta_i$ [44]. this adjustment causes the baseline predicted ratings to be closer to global mean when the user or item has few ratings. Funk [44] found that **25** was a useful value for the damping terms.\n",
    "\n",
    "------------------\n",
    "\n",
    "$b_u = \\frac{1}{|I_u| + \\beta_u} \\sum_{i \\in I_u} (r_{u,i} - \\mu)$\n",
    "\n",
    "$b_i = \\frac{1}{|U_i| + \\beta_i} \\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu)$\n",
    "\n",
    "$\\beta_u = \\beta_i = 25$\n",
    "\n",
    "-------------\n",
    "\n",
    "* additional baselines can be computed and added, and the baselines can be made more sophisticated to deal with various effects [14, 80, 115].\n",
    "\n",
    "* if an item or user has no ratings, its baseline can be set to **0**, effectively assuming that it is an average user or item.\n",
    "\n",
    "* baseline predictors effectively capture effects of **user bias**, **item popularity**, and can be applied to more exotic but increasingly-important factors such as time [80, 115].\n",
    "\n",
    "* if the baseline is subtracted from the $\\textbf{R}$ to yield a **normalised ratings matrix** $\\hat{\\textbf{R}}$, all that remains for CF to do is to efficiently capture the interaction effect between users and items. Further, the missing values of $\\hat{\\textbf{R}}$ are **0** rather than unknown, simplifying some computations and allowing the matrix to be handled by standard sparse matrix packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Normalized Ratings Matrix $\\hat{\\textbf{R}}$ for MovieLens 1M Dataset\n",
    "\n",
    "* missing values of data should be assigned to 0.\n",
    "\n",
    "\n",
    "* if no rating exists for a user or an item, corresponding baseline ($b_u, b_i$) should be assigned to 0. This is not a case in this dataset.\n",
    "\n",
    "\n",
    "* this is preprocessing step to calculate similarity matrix for **real-valued** rating domain. If rating domain is **unary**, to implement appropriate preprocessing algorithm, look at user-user CF or item-item CF sections. \n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "1. calculate baselines with damping terms $\\beta_u = \\beta_i = 25$ and obtain normalized ratings matrix.\n",
    "2. calculate baselines with damping terms and obtain normalized ratings matrix after adding a new user and a new item without rating values to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = np.mean([d for d in dat.flatten() if d != 0])\n",
    "b_u = np.array([np.mean([u_i_r - mu for u_i_r in u if u_i_r != 0]) for u in dat])\n",
    "b_i = np.array([np.mean([i_u_r - mu - b_u[u_id] for u_id, i_u_r in enumerate(i) if i_u_r != 0]) for i in dat.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_model = np.zeros(dat.shape, dtype='float')\n",
    "\n",
    "for u in xrange(baseline_model.shape[0]):\n",
    "    for i in xrange(baseline_model.shape[1]):\n",
    "        if dat[u, i] == 0:\n",
    "            baseline_model[u, i] = 0\n",
    "        else:\n",
    "            baseline_model[u, i] = mu + b_u[u] + b_i[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prediction for User ID 1 and Movie ID 2\n",
      "Original Rating : 0\n",
      "Predicted Rating : 3.86453658693\n",
      "\n",
      "** Prediction for User ID 6 and Movie ID 2\n",
      "Original Rating : 0\n",
      "Predicted Rating : 3.57726579235\n"
     ]
    }
   ],
   "source": [
    "# To predict a missing value with baseline model calculate \"mu + b_u[user_index] + b_i[item_index]\"\n",
    "\n",
    "print '** Prediction for User ID {} and Movie ID {}'.format(user_ids[0], movie_ids[1])\n",
    "print 'Original Rating : {}'.format(dat[0][1])\n",
    "print 'Predicted Rating : {}'.format(mu + b_u[0] + b_i[1])\n",
    "print '\\n** Prediction for User ID {} and Movie ID {}'.format(user_ids[5], movie_ids[1])\n",
    "print 'Original Rating : {}'.format(dat[5][1])\n",
    "print 'Predicted Rating : {}'.format(mu + b_u[5] + b_i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_ratings = dat.astype('float') - baseline_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-User Collaborative Filtering (UUCF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Item Collaborative Filtering (IICF)\n",
    "\n",
    "* UUCF, while effective, suffers from **scalability** problems as the user base grows. searching for the neighbours of a user is an $O(|U|)$ operation (or worse, depending on how similarities are computing $-$ directly computing most similarity functions against all other users is linear in the total number of ratings). IICF [71, 87, 130] takes a major step in this direction and is one of the most widely deployed CF techniques today.\n",
    "\n",
    "------------------\n",
    "\n",
    "* UUCF : uses similarities between users' rating behaviour to predict preferences.\n",
    "\n",
    "\n",
    "* IICF : uses similarities between the rating patterns of items to predict preferences. if two items tend to have the same users like and dislike them, then they are similar and users are expected to have similar preferanes for similar items.\n",
    "\n",
    "------------------\n",
    "\n",
    "* similar to **Content-Based Filtering** for recommendation and presonalization, but item similarity is deduced from user preference patterns rather than extracted from item data.\n",
    "\n",
    "* necessary to find the most similar items (solving k-NN problem) to generate predictions and recommendations. If $|U| >> |I|$, then it allows the neighbourhood-finding to be amongst the smaller of the two dimensions, but this is a **small gain**.\n",
    "\n",
    "* provides major performance gains by lending itself well to **pre-computing the similarity matrix**.\n",
    "\n",
    "---------------------\n",
    "\n",
    "* UUCF : as a user rates and re-rates items, their rating vector will change along with their similarity to other users. finding similar users in advance is therefore complicated: a user's neighbourhood is determined not only by their ratings but also by the ratings of other users, so their neighbourhood can change as a result of new ratings supplied by any user in the system. for this reason, most UUCF systems **find neighbourhoods at the time** when predictions or recommendations are needed.\n",
    "\n",
    "\n",
    "* IICF : in systems with a sufficiently high $\\frac{|U|}{|I|}$, one user adding or changing ratings is unlikely to significantly change the similarity between two items, particularly when the items have many ratings. therefore, it is reasonable to **pre-compute similarities between items** in an item-item similarity matrix. the rows of this matrix can even be **truncated to only store the** $k$ **most similar items**. As users change ratings, this data will become **slightly stale**, but the users will likely still receive good recommendations and the data can be **fully updated by re-computing the similarities** during a **low-load time** for the system.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "* UUCF : generates predictions by using **other users' ratings for the target item** combined with **user similarities**.\n",
    "\n",
    "\n",
    "* IICF : generates predictions by using the **user's own ratings for other items** combined with **those items' similarities to the target item**.\n",
    "\n",
    "-------------------------\n",
    "\n",
    "* system needs;\n",
    "\n",
    "    1. a similarity function $s : I$ x $I$ \n",
    "    2. a method to generate predictions from ratings and similarities.\n",
    "    \n",
    "### Computing Predictions \n",
    "\n",
    "* in **real-valued ratings domains**, the similarity scores can be used to generate predictions using a **weighted average**.\n",
    "\n",
    "* recommendations are generated by picking the candidate items with the highest predictions.\n",
    "\n",
    "-----------------\n",
    "\n",
    "$S$ : set of items similar to item $i$. \n",
    "\n",
    "$p_{u,i}$ : predicted value\n",
    "\n",
    "----------------------\n",
    "\n",
    "$p_{u,i} = \\frac{\\sum_{j \\in S} s(i,j) r_{u,j}}{\\sum_{j \\in S} |s(i,j)|}$\n",
    "\n",
    "* $S$ is typically selected to be the $k$ items most similar to $i$ that $u$ has also rated for some neighbourhood size $k$. Sarwar et al. [130] found $k = 30$ produced good results on the **MovieLens** dataset.\n",
    "\n",
    "\n",
    "* this equation suffers from two deficiencies;\n",
    "    1. when it is possible for similarity scores to be **negative** and ratings are constrained to be **non-negative**, some of the ratings averaged to compute the prediction may be **negative** after weightings. while this will **not affect the relative ordering of items** by predicted value, it will **bias the predicted values so they no longer map back to the user rating domain**. this can be corrected either by **thresholding** similarities so only items with **non-negative** similarities are considered or by **averaging** distance from the baseline predictor. in latter case, above equation becomes;\n",
    "    \n",
    "    $p_{u,i} = \\frac{\\sum_{j \\in S} s(i,j) (r_{u,j} - b_{u,i})}{\\sum_{j \\in S} |s(i,j)|} + b_{u,i}$\n",
    "    \n",
    "    2. when rating scales are **non-real-valued** - particularly **unary** scales are common on e-commerce sites without ratings data - the averaging does not work. if all similarities are positive and $r_{u,i} = 1$ if user $u$ has purchased item $i$, then base equation to compute predictions always evaluates to **1**. Also, with negative similarities, it is similarly **ill-behaved**. to work around this, we can compute **pseudo-predictions** $\\tilde{p}_{u,i}$ with a simple aggregation of the similarities to items in the user's purchase history $I_u$. **Summation** has been tested and shown to perform well; other aggregations such as **mean** or **max** may also be considered or used in practice.\n",
    "    \n",
    "    $\\tilde{p}_{u,i} = \\sum_{j \\in I_u} s(i, j)$\n",
    "    \n",
    "    $\\tilde{p}_{u,i}$ is not in a meaningful scale to predict any particular user behaviour, but the predict task is typically not as important in unary contexts. This preudo-prediction can, however, be used to **rank** candidate items for recommendation, forming a good basis for using IICF to recommend items based on user purchase histories [38, 71, 87].\n",
    "\n",
    "### Computing Item Similarity\n",
    "\n",
    "* $\\textbf{S}$ : item-item similarity matrix. $\\textbf{S}$ is a standard **sparse matrix**, with missing values being **0** (no similarity); it differs in this respect from $\\textbf{R}$, where missing values are unknown.\n",
    "\n",
    "#### Cosine similarity\n",
    "\n",
    "* cosine similarity between item rating vectors is the most popular similarity metric, as it is **simple**, **fast**, and produces good predictive accuracy;\n",
    "\n",
    "$s(i,j) = \\frac{\\textbf{r}_i \\cdot \\textbf{r}_i}{||\\textbf{r}_i||_2 ||\\textbf{r}_j||_2}$\n",
    "\n",
    "#### Conditional Probability [TODO]\n",
    "\n",
    "* for domains with **unary** ratings (such as shopping site purchase histories), Karypis [71] proposed a similarity function based on conditional probabilities: \n",
    "\n",
    "#### Pearson Correlation\n",
    "\n",
    "* pearson correlation has also been proposed for item-item recommendation, but does not seem to work as well as cosine similarity [130].\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "* preprocessing:\n",
    "\n",
    "    * in order to optimize the recommender's performance, it is important to **normalise** ratings** $\\textbf{R}$ **prior to computing the similarity matrix [14, 130]. \n",
    "    \n",
    "    * in **real-valued** domains variation in ratings due to **user ratings bias** (e.g., two users liking and disliking similar films, but one being a cynic who rates average films 2.5/5 and the other an enthusiast who rates the average film at 4/5) and **item bias** allows the CF to focus on the more nuanced differences in user preference for particular items. this can be accomplished by **subtracting a baseline predictor from all ratings prior to computing similarities** (e.g., compute similarities over ratings $\\hat{r}_{u,i} = r_{u,i} - \\mu - b_u - b_i$).\n",
    "\n",
    "    * when applying **cosine similarity** in **unary** ratings domains, it can be useful to **normalise each user's ratings vector** $r_u$ **to the unit vector prior to computing item similarities**. the effect of this adjustment is that users who have purchased fewer items have more impact on the similarity of the items they have purchased than users who have purchased many items [38, 71].\n",
    "    \n",
    "    \n",
    "* postprocessing:\n",
    "\n",
    "    * **normalising item similarity vectors** (rows of $\\textbf{S}$) **to unit vectors** can be beneficial. this causes items with **sparser neighbourhoods** (fewer similar items) to have more influence in computing the final predictions [38, 71].\n",
    "    * [TODO] Question: normalisation does not affect predictions as can be seen in prediction equations.\n",
    "\n",
    "### Pre-computing and Truncating the Model\n",
    "\n",
    "* due to the relatively static nature of item similarities when $|U| >> |I|$, it is feasible to **pre-compute** item-item similarities and **cache the** $k'$ **most similar items to each item**. prediction can then be performed quickly by looking up the similarity list for each item rated by the current user and aggregating their similarities into a predicted preference.\n",
    "\n",
    "* caching more items than are used in the similarity computation (so $k' > k$) is useful to increase the likelihood of having $k$ similar items after items already rated by the user have been removed from the candidate set.\n",
    "\n",
    "* **pre-computation** and **truncation** is essential to depoloying collaborative filtering in practice, as it **places an upper bound on the number of items which must be considered to produce a recommendation** and **eliminates the query-time cost of similarity computation**. itcomes with the small expense of **reducing the number of items for which predictions can be generated** (the coverage of the recommender), but the unrecommendable items will usually have low predicted preferences anyway. \n",
    "\n",
    "|Process                         |**Real-Valued** Rating Domain                  |**Unary** Rating Domain|\n",
    "|:------------------------------:|:---------------------------------------------:|:-----:|\n",
    "| Normalisation of $\\textbf{R}$  | using **baseline predictor** in first section |  **normalise** each user's ratings vector $r_u$ to the **unit vector** (if **cosine similarity** or **pearson correlation** is used)|\n",
    "| Postprocessing of $\\textbf{S}$ | **normalising** item similarity vectors (rows of $\\textbf{S}$) to **unit vectors**    | **normalising** item similarity vectors (rows of $\\textbf{S}$) to **unit vectors**|\n",
    "| Similarity Metrics             | **cosine similarity** or **pearson correlation** | **cosine similarity**, **pearson correlation** or **conditional probability**|\n",
    "| Computing Predictions | $p_{u,i} = \\frac{\\sum_{j \\in S} s(i,j) (r_{u,j} - b_{u,i})}{\\sum_{j \\in S} |s(i,j)|} + b_{u,i}$ (or **thresholding** instead of **averaging distance** from base predictor)| $\\tilde{p}_{u,i} = \\sum_{j \\in I_u} s(i, j)$ (**mean** or **max** can be used instead of **summation**)|\n",
    "| Selection of $k$ | -- | -- |\n",
    "| Selection of $k'$ when **truncating** | -- | -- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Similarity Matrix $\\textbf{S}$ for MovieLens 1M Dataset and Recommendation\n",
    "\n",
    "* missing values of similarity matrix will be 0, which represents no similarity.\n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "1. use pearson correlation instead of cosine similarity as similarity metric.\n",
    "2. truncate similarity matrix to appropriate $k'$ items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Similarity Matrix $\\textbf{S}$ and Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarity_matrix = iicfsm.get_similarity_matrix(normalized_ratings)\n",
    "similarity_matrix = iicfsm.normalize_similarity_matrix(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Rating of a User for an Item\n",
    "\n",
    "* Sarwar et al. [130] found k=30 produced good results on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(predict_user_index, predict_item_index):\n",
    "    \n",
    "    k_most_similar = 30\n",
    "    \n",
    "    user_id = user_ids[predict_user_index]\n",
    "    movie_id = movie_ids[predict_item_index]\n",
    "\n",
    "    original_rating = dat[predict_user_index][predict_item_index]\n",
    "\n",
    "    avg_dist_prediction = iicfp.predict_with_avg_distance(predict_user_index, predict_item_index, dat, \n",
    "                                                          similarity_matrix, k_most_similar, baseline_model)\n",
    "\n",
    "    thresholded_prediction = iicfp.predict_with_thresholding(predict_user_index, predict_item_index, dat, \n",
    "                                                             similarity_matrix, k_most_similar)\n",
    "\n",
    "    print '\\n** Prediction for User ID / Movie ID                            : {} / {}'.format(user_id, movie_id)\n",
    "    print '** Original Rating                                              : {}'.format(original_rating)\n",
    "    print '-- Prediction by Averaging Distance from the Baseline Predictor : {}'.format(avg_dist_prediction)\n",
    "    print '-- Prediction by Thresholding Similarities                      : {}'.format(thresholded_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Most (Dis-)Simillar Items to defined Item Using Similarity Matrix and Predict Ratings of those Items for defined User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sim_n_dissim(predict_item_index):\n",
    "    \n",
    "    movie_id = movie_ids[predict_item_index]\n",
    "\n",
    "    most_similar_item, similarity_value = iicfp.get_similar_items_using_similarity_matrix(predict_item_index, \n",
    "                                                                                          similarity_matrix, 1)\n",
    "    most_similar_item = most_similar_item[0]\n",
    "    similarity_value = similarity_value[0]\n",
    "    most_similar_movie_id = movie_ids[most_similar_item]\n",
    "\n",
    "    most_dissimilar_item, dis_similarity_value = iicfp.get_dissimilar_items_using_similarity_matrix(predict_item_index, \n",
    "                                                                                                    similarity_matrix, 1)\n",
    "    most_dissimilar_item = most_dissimilar_item[0]\n",
    "    dis_similarity_value = dis_similarity_value[0]\n",
    "    most_dissimilar_movie_id = movie_ids[most_dissimilar_item]\n",
    "\n",
    "    print '\\n** Prediction for Movie ID {}'.format(movie_id)\n",
    "    print '-- Most Similar Movie Index    : {:6d}, Movie ID : {:6d}, Similarity Value : {}'.format(most_similar_item,\n",
    "                                                                                             most_similar_movie_id,\n",
    "                                                                                             similarity_value)\n",
    "    print '-- Most Dissimilar Movie Index : {:6d}, Movie ID : {:6d}, Similarity Value : {}'.format(most_dissimilar_item,\n",
    "                                                                                             most_dissimilar_movie_id,\n",
    "                                                                                             dis_similarity_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Prediction for User ID / Movie ID                            : 322 / 266\n",
      "** Original Rating                                              : 5\n",
      "-- Prediction by Averaging Distance from the Baseline Predictor : 4.56400745677\n",
      "-- Prediction by Thresholding Similarities                      : 4.56400745677\n",
      "\n",
      "** Prediction for Movie ID 266\n",
      "-- Most Similar Movie Index    :   2668, Movie ID :   2875, Similarity Value : 0.064064557291\n",
      "-- Most Dissimilar Movie Index :   1060, Movie ID :   1138, Similarity Value : -0.0464289192239\n"
     ]
    }
   ],
   "source": [
    "predict(321, 259)\n",
    "sim_n_dissim(259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Prediction for User ID / Movie ID                            : 322 / 2875\n",
      "** Original Rating                                              : 0\n",
      "-- Prediction by Averaging Distance from the Baseline Predictor : 4.47743707569\n",
      "-- Prediction by Thresholding Similarities                      : 4.55924405334\n",
      "\n",
      "** Prediction for User ID / Movie ID                            : 322 / 1138\n",
      "** Original Rating                                              : 0\n",
      "-- Prediction by Averaging Distance from the Baseline Predictor : -1.93244620498\n",
      "-- Prediction by Thresholding Similarities                      : 4.60015785374\n"
     ]
    }
   ],
   "source": [
    "predict(321, 2668)\n",
    "predict(321, 1060)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "* an item is a $|U|$-dimensional vector with missing values of users' preferences for it (similarly, a user is a $|I|$-dimensional vector). there is redundancy in these dimensions, as both users and items will usually be divisible into groups with similar preference profiles. it is therefore natural to ask whether the dimensionality of the rating space can be reduced - **can we find a smaller number of dimensions, ideally a constant number $k$, so that items and users can be represented by $k$-dimensional vectors**?\n",
    "\n",
    "### Defining Singular Value Decomposition (SVD)\n",
    "\n",
    "* for a matrix $\\textbf{M}$, its SVD is the factorisation of $\\textbf{M}$ into three constituent matrices such that $\\textbf{M} = \\textbf{U}\\Sigma\\textbf{T}^T$. $\\Sigma$ is a **diagonal matrix** whose values $\\sigma_i$ are the **singular values** of the decomposition, and both $\\textbf{U}$ and $\\textbf{T}$ are **orthogonal**. this decomposition introduces an intermediate vector space represented by $\\Sigma$.\n",
    "\n",
    "\n",
    "* if $\\textbf{M}$ is the **ratings matrix**, $\\Sigma\\textbf{T}^T$ transforms vectors from **item-space** into the **intermediate vector space**.\n",
    "\n",
    "\n",
    "* in the pure form of the SVD\n",
    "    * $\\textbf{M}$ : [m x n] and has rank $\\hat{k}$\n",
    "    * $\\textbf{U}$ : [m x $k$]\n",
    "    * $\\Sigma$     : [k x $\\hat{k}$]\n",
    "    * $\\textbf{T}$ : [n x $\\hat{k}$]\n",
    "\n",
    "\n",
    "* $\\Sigma$ can be **truncated** by only retaining the $k$ largest singular values to yield $\\Sigma_k$. resulting decomposition is an approximation of $\\textbf{M}$. further, using the **Frobenius norm as the measure of error**, it is the best possible **rank-$k$ approximation** [36].\n",
    "\n",
    "\n",
    "* this truncation simultaneously achieves two goals;\n",
    "    1. it **decreases the dimensionality** of the vector space, **decreasing the storage** and **computational requirements** for the model. items and users can each be represented by $k$-dimensional vectors.\n",
    "    2. by dropping the **smaller singular values**, small preturbances as a result of **noise in the data are eliminated**, leaving only the **strongest effects** or **trends** in the model.\n",
    "    \n",
    "\n",
    "* computing the SVD of the ratings matrix results in the following factorisation ($m = |U|$, $n = |I|$, and $\\sigma$ : [$k$ x $k$] diagonal matrix)\n",
    "\n",
    "$\\textbf{R} \\approx \\textbf{U}\\Sigma_k\\textbf{T}^T$\n",
    "\n",
    "\n",
    "* the rows of the $|U|$ x $k$ matrix $\\textbf{U}$ are the users' interest in each of the $k$ **inferred topics (inferred items)**, and the rows of $\\textbf{T}$ are the item's relevance for each **inferred topic**. the singular values in $\\Sigma$ are **weights for the preferences**, representing the influence of a particular **inferred topic** on user-item preferences across the system.\n",
    "\n",
    "\n",
    "* a user's preference for an item, therefore, is the **weighted sum of the user's interest in each of the inferred topics times that item's relevance to the inferred topic**.\n",
    "\n",
    "### Computing and Updating the SVD\n",
    "\n",
    "* in order to use SVD (or any matrix factorisation method), it is necessary to first compute the matrix factorisation. there are a variety of algorithms for computing SVD, including **Lanczos' algorithm**, **the generalized Hebian algorithm**, and **expectation maximisation** [51, 81, 127].\n",
    "\n",
    "\n",
    "* SVD is only **well-defined** when the matrix is **complete**. Therefore, to factor the rating matrix, the missing values must be filled with some reasonable default (a method called **imputation**). Sarwar et al. [131] found the **item's average rating** to be a useful default value (they tried user average as well, but item average performed better). Alternatively, the SVD can be computed over the **normalised ratings matrix** $\\hat{\\textbf{R}}$ and the missing values considered to be **0**.\n",
    "\n",
    "\n",
    "* several methods have been proposed that compute an estimate of the **SVD only on the known ratings**, dropping the requirement to impute or otherwise account for missing ratings. Kurucz et al. [81] propose a **least-squares method** that learns a regression for each user. another method that has become quite **popular in the last few years** is **gradient descent** [44, 110]. this method **trains** each topic $f$ in turn, using the following **update rules**;\n",
    "\n",
    "$\\lambda = 0.001$ (learning rate)\n",
    "\n",
    "$\\Delta u_{j,f} = \\lambda (r_{u,i} - p_{u,i}) i_{k,f}$\n",
    "\n",
    "$\\Delta i_{k,f} = \\lambda (r_{u,i} - p_{u,i}) u_{j,f}$\n",
    "\n",
    "\n",
    "* the gradient descent method for estimating the SVD also allows for **regularisation** to prevent **overfitting** the resulting model. The resulting model will not be a **true** SVD of the rating matrix, as the component matrices are **no longer orthogonal**, but tends to be **more accurate** at predicting unseen preferences than the unregularised SVD. the regularisation is accomplished by adding an additional term to the update rules. $\\gamma$ is the **regularisation factor**, typically $0.1$-$0.2$.\n",
    "\n",
    "$\\Delta u_{j,f} = \\lambda ((r_{u,i} - p_{u,i}) i_{k,f} - \\gamma u_{j,f})$\n",
    "\n",
    "$\\Delta i_{k,f} = \\lambda ((r_{u,i} - p_{u,i}) u_{j,f} - \\gamma i_{k,f})$\n",
    "\n",
    "* prior to computing the SVD, the ratings can additionally be **normalised**, by subtracting the **user's mean rating** or **some other baseline predictor**. this can **improve both accuracy** [131] and **accelerate convergence of iterative methods**.\n",
    "\n",
    "\n",
    "* once the SVD is computed, it is necessary to **update** it to reflect **new users**, **items**, and **ratings**. a commonly used method for updating the SVD is **folding-in**; it works well in practive and **allows users who were not considered when the ratings matrix was factored to receiver recommendations and predictions** [16, 129]. **Folding-in** operates by computing a **new user-preference** or **topc-relevance** vector for the new user or item but not recomputing the decomposition itself.\n",
    "\n",
    "\n",
    "* for a user $u$, **folding-in** computes their topic interest vector $u$ such that\n",
    "\n",
    "\n",
    "### Generating Predictions \n",
    "\n",
    "### Computing Similarities\n",
    "\n",
    "### Normalisation in SVD\n",
    "\n",
    "### Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
